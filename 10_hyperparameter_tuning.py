import pandas as pd
import numpy as np
import os
import json
import optuna
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# ==========================================
# 1. æ•°æ®å‡†å¤‡å‡½æ•° (ä¿®å¤ç‰ˆï¼šå…ˆç­›é€‰åˆ—ï¼Œå†dropna)
# ==========================================
def prepare_data(df, ticker, horizon_days, look_back=60):
    # 1. ç­›é€‰ç‰¹å®š Ticker çš„æ•°æ®
    data = df[df['Ticker'] == ticker].copy()
    
    if len(data) == 0:
        # Ticker ä¸å­˜åœ¨
        return None, None, None, None, None

    # 2. ç¡®ä¿æŒ‰æ—¶é—´æ’åº
    if 'Date' in data.columns:
        data['Date'] = pd.to_datetime(data['Date'])
        data = data.sort_values('Date')

    # 3. å®šä¹‰ç‰¹å¾åˆ— (è¯·ç¡®ä¿è¿™äº›åˆ—ååœ¨ä¸‹é¢çš„ã€åˆ—åä¾¦æ¢ã€‘è¾“å‡ºä¸­èƒ½æ‰¾åˆ°)
    # æ ¹æ®ä½ çš„æˆªå›¾ï¼Œæˆ‘çŒœæµ‹åˆ—åå¦‚ä¸‹ï¼Œå¦‚æœæŠ¥é”™ï¼Œè¯·çœ‹æ§åˆ¶å°è¾“å‡ºçš„å®é™…åˆ—å
    feature_cols = [
        'Open', 'High', 'Low', 'Close', 'Volume', 
        'RSI_14', 'MACD_12_26_9', 'MACDs_12_26_9' 
    ]
    
    # ã€æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ã€‘
    missing_cols = [c for c in feature_cols if c not in data.columns]
    if missing_cols:
        print(f"   âš ï¸ [è·³è¿‡] {ticker} ç¼ºå°‘åˆ—: {missing_cols}")
        return None, None, None, None, None

    # 4. åˆ›å»ºç›®æ ‡å˜é‡
    data['Target'] = (data['Close'].shift(-horizon_days) > data['Close']).astype(int)
    
    # =========================================================
    # ã€æ ¸å¿ƒä¿®å¤ã€‘ åªä¿ç•™æˆ‘ä»¬éœ€è¦çš„åˆ—ï¼Œé˜²æ­¢è¢«æ— å…³åˆ—çš„ç©ºå€¼è¯¯æ€
    # =========================================================
    needed_cols = feature_cols + ['Target']
    data = data[needed_cols] 

    # 5. å»é™¤ç©ºå€¼
    rows_before = len(data)
    data = data.dropna()
    rows_after = len(data)

    if rows_after == 0:
        print(f"   âš ï¸ [è·³è¿‡] {ticker} dropna() åä¸ºç©º (åŸ: {rows_before} -> 0)ã€‚è¯·æ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦å…¨æ˜¯NaNã€‚")
        return None, None, None, None, None

    # 6. æ•°æ®å½’ä¸€åŒ–
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data[feature_cols]) 

    # 7. æ„å»º LSTM åºåˆ—
    X, y = [], []
    for i in range(look_back, len(scaled_data)):
        X.append(scaled_data[i-look_back:i])
        y.append(data['Target'].iloc[i])

    X, y = np.array(X), np.array(y)

    if len(X) == 0:
        print(f"   âš ï¸ [è·³è¿‡] {ticker} æ„å»ºåºåˆ—åæ•°æ®ä¸è¶³ (è¡Œæ•° < look_back)")
        return None, None, None, None, None

    # 8. åˆ’åˆ†æ•°æ®é›†
    split = int(len(X) * 0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    return X_train, y_train, X_test, y_test, scaler

# ==========================================
# 2. æ¨¡å‹æ„å»ºå‡½æ•°
# ==========================================
def create_lstm_model(trial, input_shape):
    model = Sequential()
    units_1 = trial.suggest_int('units_1', 32, 128)
    return_sequences = trial.suggest_categorical('return_sequences', [True, False])
    dropout_1 = trial.suggest_float('dropout_1', 0.1, 0.5)
    
    model.add(LSTM(units=units_1, return_sequences=return_sequences, input_shape=input_shape))
    model.add(Dropout(dropout_1))
    
    if return_sequences:
        units_2 = trial.suggest_int('units_2', 16, 64)
        dropout_2 = trial.suggest_float('dropout_2', 0.1, 0.5)
        model.add(LSTM(units=units_2, return_sequences=False))
        model.add(Dropout(dropout_2))
    
    model.add(Dense(1, activation='sigmoid'))
    
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# ==========================================
# 3. ä¸»ä¼˜åŒ–æµç¨‹
# ==========================================
def run_optimization():
    current_dir = os.getcwd()
    DATA_PATH = os.path.join(current_dir, 'data', 'finalized', 'final_master_dataset.csv')
    RESULTS_DIR = os.path.join(current_dir, 'data', 'tuned_models') 
    
    if not os.path.exists(DATA_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {DATA_PATH}")
        return

    os.makedirs(RESULTS_DIR, exist_ok=True)

    print(f"æ­£åœ¨è¯»å–æ•°æ®: {DATA_PATH} ...")
    df = pd.read_csv(DATA_PATH)
    
    # æ¸…æ´— Ticker
    if 'Ticker' in df.columns:
        df['Ticker'] = df['Ticker'].astype(str).str.strip()
    
    # ==========================================
    # ğŸ•µï¸ åˆ—åä¾¦æ¢ï¼šæ‰“å°å‡ºæ‰€æœ‰åˆ—åï¼Œæ–¹ä¾¿æ ¸å¯¹
    # ==========================================
    print("\n" + "="*40)
    print("ğŸ•µï¸  åˆ—åä¾¦æ¢æŠ¥å‘Š (è¯·æ ¸å¯¹MACDåˆ—åæ˜¯å¦ä¸€è‡´):")
    print(df.columns.tolist())
    print("="*40 + "\n")

    tickers = df['Ticker'].unique()
    print(f"æ£€æµ‹åˆ° {len(tickers)} ä¸ªè‚¡ç¥¨ã€‚")
    
    # âš ï¸ è°ƒè¯•æ¨¡å¼ï¼šåªè·‘å‰ 2 ä¸ªè‚¡ç¥¨ã€‚å¦‚æœæˆåŠŸäº†ï¼ŒæŠŠ [:2] å»æ‰æ”¹æˆ tickers
    target_tickers = tickers[:2]  
    
    horizons = {'Short': 1, 'Medium': 5, 'Long': 20}
    best_params_record = {}

    for ticker in target_tickers:
        for horizon_name, horizon_days in horizons.items():
            print(f"\n>>> æ­£åœ¨å¤„ç†: {ticker} - {horizon_name} ...")

            X_train, y_train, X_test, y_test, scaler = prepare_data(df, ticker, horizon_days)

            if X_train is None:
                continue

            # å®šä¹‰ Optuna ç›®æ ‡å‡½æ•°
            def objective(trial):
                model = create_lstm_model(trial, (X_train.shape[1], X_train.shape[2]))
                early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
                history = model.fit(
                    X_train, y_train,
                    validation_split=0.2,
                    epochs=5,  # è°ƒè¯•ç”¨ 5ï¼Œæ­£å¼è·‘æ”¹ 10-20
                    batch_size=32,
                    callbacks=[early_stopping],
                    verbose=0
                )
                return max(history.history['val_accuracy'])

            # è°ƒè¯•ç”¨ 2 æ¬¡ trialï¼Œæ­£å¼è·‘æ”¹ 10-20
            study = optuna.create_study(direction='maximize')
            study.optimize(objective, n_trials=2) 

            print(f"   âœ… æˆåŠŸ! æœ€ä½³å‡†ç¡®ç‡: {study.best_value:.4f}")

            key = f"{ticker}_{horizon_name}"
            best_params_record[key] = {
                'best_params': study.best_params,
                'best_accuracy': study.best_value
            }

    # --- ä¿å­˜ç»“æœ ---
    print("\n" + "="*30)
    if not best_params_record:
        print("âŒ ä¾ç„¶æ²¡æœ‰ç”Ÿæˆç»“æœã€‚è¯·æ£€æŸ¥ä¸Šæ–¹æŠ¥é”™ä¿¡æ¯ã€‚")
    else:
        params_file = os.path.join(RESULTS_DIR, 'best_hyperparameters.json')
        with open(params_file, 'w') as f:
            json.dump(best_params_record, f, indent=4)
        print(f"âœ… å®Œç¾ï¼ç»“æœå·²ä¿å­˜è‡³: {params_file}")

if __name__ == "__main__":
    run_optimization()