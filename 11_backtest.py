import os
import pandas as pd
import numpy as np
import json
import torch
import torch.nn as nn
import torch.optim as optim
import math
from sklearn.preprocessing import StandardScaler

# ==========================================
# 1. é…ç½®ä¸å‚æ•°
# ==========================================
DATA_PATH = 'data/finalized/data_with_targets.csv'
PARAMS_PATH = 'models/tuned_models/best_hyperparameters.json'
RESULTS_DIR = 'backtest_results'  # ç¡®ä¿ç»“æœç›®å½•æ­£ç¡®
SEQ_LENGTH = 60

# äº¤æ˜“ç­–ç•¥é…ç½®
INITIAL_CAPITAL = 10000
CONFIDENCE_THRESHOLD = 0.05 

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
os.makedirs(RESULTS_DIR, exist_ok=True)

# ==========================================
# 2. æ¨¡å‹å®šä¹‰ (ä¿æŒä¸å˜)
# ==========================================
class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return self.sigmoid(out)

class GRUClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout):
        super(GRUClassifier, self).__init__()
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        out, _ = self.gru(x)
        out = self.fc(out[:, -1, :])
        return self.sigmoid(out)

class TransformerClassifier(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers, dropout):
        super(TransformerClassifier, self).__init__()
        self.input_embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=d_model*2, dropout=dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
        self.fc = nn.Linear(d_model, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, src):
        src = self.input_embedding(src)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = self.fc(output[:, -1, :])
        return self.sigmoid(output)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

# ==========================================
# 3. è¾…åŠ©å‡½æ•°
# ==========================================
def create_sequences(data_x, data_y, prices, dates, seq_length):
    xs, ys, ps, ds = [], [], [], []
    for i in range(len(data_x) - seq_length):
        xs.append(data_x[i:(i + seq_length)])
        ys.append(data_y[i + seq_length])
        ps.append(prices[i + seq_length])
        ds.append(dates[i + seq_length]) # åŒæ—¶ä¿å­˜æ—¥æœŸ
    return np.array(xs), np.array(ys), np.array(ps), np.array(ds)

def prepare_data_split(df, ticker, horizon_days):
    t_df = df[df['Ticker'] == ticker].copy()
    if 'Date' in t_df.columns:
        t_df['Date'] = pd.to_datetime(t_df['Date'])
        t_df = t_df.sort_values('Date')
    
    price_col = 'Close' if 'Close' in t_df.columns else t_df.select_dtypes(include=[np.number]).columns[0]
    
    # ç”Ÿæˆ Target
    t_df['Target'] = (t_df[price_col].shift(-horizon_days) > t_df[price_col]).astype(float)
    t_df = t_df.dropna(subset=['Target'])
    
    feature_cols = [c for c in t_df.columns if c not in ['Date', 'Ticker', 'Target'] and not c.startswith('target_')]
    numeric_cols = t_df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [c for c in feature_cols if c in numeric_cols]
    
    raw_x = t_df[feature_cols].values
    raw_y = t_df['Target'].values
    raw_prices = t_df[price_col].values
    raw_dates = t_df['Date'].values # è·å–åŸå§‹æ—¥æœŸ
    
    # åˆ‡åˆ† Train/Test
    split_idx = int(len(raw_x) * 0.8)
    
    train_x_raw = raw_x[:split_idx]
    test_x_raw = raw_x[split_idx:]
    
    train_y = raw_y[:split_idx]
    test_y = raw_y[split_idx:]
    
    train_prices = raw_prices[:split_idx]
    test_prices = raw_prices[split_idx:]
    
    train_dates = raw_dates[:split_idx]
    test_dates = raw_dates[split_idx:]
    
    # Scaling
    scaler = StandardScaler()
    train_x_scaled = scaler.fit_transform(train_x_raw)
    test_x_scaled = scaler.transform(test_x_raw)
    
    # ç”Ÿæˆåºåˆ— (æ³¨æ„è¿™é‡Œå¢åŠ äº† dates è¿”å›)
    X_train, y_train, _, _ = create_sequences(train_x_scaled, train_y, train_prices, train_dates, SEQ_LENGTH)
    X_test, y_test, prices_test, dates_test = create_sequences(test_x_scaled, test_y, test_prices, test_dates, SEQ_LENGTH)
    
    return X_train, y_train, X_test, y_test, prices_test, dates_test, len(feature_cols)

def train_and_predict(model_cls, params, input_dim, X_train, y_train, X_test):
    model_type = params['model_type']
    dropout = params['dropout']
    
    if model_type == 'LSTM':
        model = LSTMClassifier(input_dim, params['lstm_hidden'], params['lstm_layers'], dropout).to(device)
    elif model_type == 'GRU':
        model = GRUClassifier(input_dim, params['gru_hidden'], params['gru_layers'], dropout).to(device)
    elif model_type == 'Transformer':
        d_model = params['nhead'] * params['d_model_mult']
        model = TransformerClassifier(input_dim, d_model, params['nhead'], params['tf_layers'], dropout).to(device)
    
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=params['lr'])
    
    xt = torch.FloatTensor(X_train).to(device)
    yt = torch.FloatTensor(y_train).unsqueeze(1).to(device)
    
    model.train()
    epochs = 15 # æ¼”ç¤ºç”¨ï¼Œå®é™…å¯å¢åŠ 
    for _ in range(epochs):
        optimizer.zero_grad()
        out = model(xt)
        loss = criterion(out, yt)
        loss.backward()
        optimizer.step()
        
    model.eval()
    with torch.no_grad():
        xv = torch.FloatTensor(X_test).to(device)
        preds = model(xv).cpu().numpy().flatten()
        
    return preds

# ==========================================
# 4. ä¸»å›æµ‹å¼•æ“
# ==========================================
def run_backtest_engine():
    print("ğŸš€ Starting Week 11: Backtesting Engine...")
    print(f"ğŸ“‚ Results will be saved to: {RESULTS_DIR}")
    
    if not os.path.exists(PARAMS_PATH):
        print("âŒ Best hyperparameters not found.")
        return

    with open(PARAMS_PATH, 'r') as f:
        best_params_registry = json.load(f)
        
    df = pd.read_csv(DATA_PATH)
    results = []
    
    # === æ–°å¢ï¼šä¸“é—¨ç”¨äºæ”¶é›† Mid-term æ•°æ®çš„åˆ—è¡¨ ===
    midterm_predictions = [] 
    
    for ticker, horizons in best_params_registry.items():
        for horizon_name, params in horizons.items():
            print(f"\nğŸ”„ Backtesting: {ticker} [{horizon_name}]...")
            
            horizon_days = {'Short': 1, 'Mid': 5, 'Long': 10}.get(horizon_name, 1)
            
            try:
                X_train, y_train, X_test, y_test, prices_test, dates_test, input_dim = prepare_data_split(df, ticker, horizon_days)
            except ValueError:
                print("   âš ï¸ Not enough data to split.")
                continue

            if len(X_train) < 50 or len(X_test) < 10:
                print("   âš ï¸ Not enough data.")
                continue
                
            try:
                probs = train_and_predict(None, params, input_dim, X_train, y_train, X_test)
            except Exception as e:
                print(f"   âŒ Model Error: {e}")
                continue
            
            # === å…³é”®ä¿®æ”¹ï¼šå¦‚æœæ˜¯ Mid ç­–ç•¥ï¼Œæ”¶é›†è¯¦ç»†æ•°æ®ä¾› Week 13 ä½¿ç”¨ ===
            if horizon_name == 'Mid':
                mid_df = pd.DataFrame({
                    'Date': dates_test,
                    'Ticker': ticker,
                    'Signal': probs,       # é¢„æµ‹æ¦‚ç‡
                    'Close': prices_test,  # æ”¶ç›˜ä»·
                    'Target': y_test       # çœŸå®æ ‡ç­¾
                })
                midterm_predictions.append(mid_df)
            
            # --- æ‰§è¡Œäº¤æ˜“ç­–ç•¥ ---
            cash = INITIAL_CAPITAL
            position = 0 
            trades = 0
            
            buy_thresh = 0.50 + CONFIDENCE_THRESHOLD
            sell_thresh = 0.50 - CONFIDENCE_THRESHOLD
            
            for i in range(len(probs) - 1):
                current_price = prices_test[i]
                prob = probs[i]
                
                if prob > buy_thresh and position == 0:
                    position = cash / current_price
                    cash = 0
                    trades += 1
                elif prob < sell_thresh and position > 0:
                    cash = position * current_price
                    position = 0
                    trades += 1
                
            final_price = prices_test[-1]
            final_value = cash + (position * final_price)
            roi = (final_value - INITIAL_CAPITAL) / INITIAL_CAPITAL * 100
            initial_price = prices_test[0]
            buy_hold_roi = (final_price - initial_price) / initial_price * 100
            
            pred_dirs = (probs > 0.5).astype(float)
            accuracy = (pred_dirs == y_test).mean() * 100
            
            print(f"   ğŸ’° Final: ${final_value:.2f} | ROI: {roi:.2f}% | Trades: {trades}")
            print(f"   ğŸ¯ Win Rate: {accuracy:.2f}%")
            
            results.append({
                'Ticker': ticker,
                'Horizon': horizon_name,
                'Model': params['model_type'],
                'ROI': roi,
                'Buy_Hold_ROI': buy_hold_roi,
                'Trades': trades,
                'Win_Rate': f"{accuracy:.1f}%"
            })

    # 1. ä¿å­˜å›æµ‹æ‘˜è¦
    res_df = pd.DataFrame(results)
    summary_path = os.path.join(RESULTS_DIR, 'backtest_summary.csv')
    res_df.to_csv(summary_path, index=False)
    print(f"\nâœ… Summary saved to: {summary_path}")
    
    # 2. ä¿å­˜ Week 13 æ‰€éœ€çš„ Mid-term è¯¦ç»†é¢„æµ‹æ–‡ä»¶
    if midterm_predictions:
        all_mid_df = pd.concat(midterm_predictions)
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        all_mid_df['Date'] = pd.to_datetime(all_mid_df['Date'])
        
        mid_file_path = os.path.join(RESULTS_DIR, 'daily_signals_midterm.csv')
        all_mid_df.to_csv(mid_file_path, index=False)
        print(f"âœ… [é‡è¦] Week 13 æ‰€éœ€æ–‡ä»¶å·²ç”Ÿæˆ: {mid_file_path}")
    else:
        print("âš ï¸ è­¦å‘Š: æœªç”Ÿæˆ Mid-term é¢„æµ‹æ•°æ®ï¼ŒWeek 13 å¯èƒ½ä¼šæŠ¥é”™ã€‚è¯·æ£€æŸ¥ best_hyperparameters.json æ˜¯å¦åŒ…å« 'Mid' ç­–ç•¥ã€‚")

    if not res_df.empty:
        print("\n" + "="*60)
        print(res_df[['Ticker', 'Horizon', 'Model', 'ROI', 'Buy_Hold_ROI', 'Trades', 'Win_Rate']].to_string(index=False))

if __name__ == "__main__":
    run_backtest_engine()



























































    