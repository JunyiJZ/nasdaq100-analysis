import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import os

# ==========================================
# é…ç½®é¡¹
# ==========================================
DATA_PATH = 'data/finalized/data_with_targets.csv'
MODEL_SAVE_DIR = 'models/dl_checkpoints'
RESULTS_PATH = 'baseline_results.csv' 
SEQ_LENGTH = 60   # æ—¶é—´çª—å£ï¼šç”¨è¿‡å»60å¤©é¢„æµ‹ç¬¬61å¤©
EPOCHS = 20       # è®­ç»ƒè½®æ•°
BATCH_SIZE = 32
LEARNING_RATE = 0.001

# âš ï¸ ä¿®æ­£ç‚¹ï¼šæ ¹æ®æ‚¨çš„æˆªå›¾ï¼Œè¿™é‡Œæ”¹æˆäº† 'target_1d_return'
# æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦æ”¹ä¸º 'target_5d_return' ç­‰
TARGET_COL = 'target_1d_return' 

SAMPLE_TICKERS_COUNT = 10    

# æ£€æŸ¥è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# ==========================================
# 1. æ•°æ®é¢„å¤„ç†å·¥å…·ï¼šåˆ›å»ºæ—¶é—´åºåˆ—
# ==========================================
def create_sequences(input_data, target_data, seq_length):
    """
    input_data: ç‰¹å¾æ•°æ® (N, features)
    target_data: ç›®æ ‡æ•°æ® (N,)
    è¿”å›: 
    xs: (samples, seq_length, features)
    ys: (samples,)
    """
    xs, ys = [], []
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    length = len(input_data)
    for i in range(length - seq_length):
        x = input_data[i:(i + seq_length)]
        y = target_data[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# ==========================================
# 2. å®šä¹‰ LSTM æ¨¡å‹
# ==========================================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=2):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTM å±‚
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)
        
        # å‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºç”¨äºé¢„æµ‹
        out = self.fc(out[:, -1, :])
        return out

# ==========================================
# 3. ä¸»æ‰§è¡Œé€»è¾‘
# ==========================================
def run_week9_workflow():
    print("Loading data...")
    if not os.path.exists(DATA_PATH):
        print(f"âŒ Error: æ–‡ä»¶ä¸å­˜åœ¨ -> {DATA_PATH}")
        return

    df = pd.read_csv(DATA_PATH)
    
    # --- æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨ ---
    if TARGET_COL not in df.columns:
        print(f"\nâŒ é”™è¯¯: åœ¨CSVä¸­æ‰¾ä¸åˆ°åˆ—å '{TARGET_COL}'")
        print(f"â„¹ï¸ CSVæ–‡ä»¶ä¸­çš„å¯ç”¨åˆ—åæœ‰: {df.columns.tolist()}")
        print("ğŸ‘‰ è¯·æ£€æŸ¥ä»£ç ç¬¬ 23 è¡Œçš„ TARGET_COL å˜é‡ã€‚\n")
        return
    # ----------------------------------

    # å¤„ç†æ—¥æœŸ
    if 'Date' in df.columns:
        # errors='coerce' ä¼šå°†æ— æ³•è§£æçš„æ—¥æœŸè®¾ä¸º NaTï¼Œé˜²æ­¢æŠ¥é”™
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        # åˆ é™¤æ—¥æœŸè§£æå¤±è´¥çš„è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        df = df.dropna(subset=['Date'])
        df = df.sort_values(['Ticker', 'Date'])
    
    # ç­›é€‰æ ·æœ¬è‚¡ç¥¨
    top_tickers = df['Ticker'].value_counts().head(SAMPLE_TICKERS_COUNT).index.tolist()
    print(f"Selected Sample Tickers: {top_tickers}")

    results_list = []

    for ticker in top_tickers:
        print(f"\nProcessing {ticker}...")
        
        # 1. è·å–è¯¥è‚¡ç¥¨æ•°æ®
        ticker_df = df[df['Ticker'] == ticker].copy()
        
        # å¡«å……ç¼ºå¤±å€¼
        ticker_df = ticker_df.ffill().bfill()

        # --- åŠ¨æ€é€‰æ‹©æ•°å€¼ç‰¹å¾åˆ— ---
        # æ’é™¤ Date, Ticker å’Œ æ‰€æœ‰çš„ target_ å¼€å¤´çš„åˆ—ï¼ˆé˜²æ­¢æ•°æ®æ³„éœ²ï¼‰
        feature_cols = [c for c in ticker_df.columns if c not in ['Date', 'Ticker'] and not c.startswith('target_')]
        
        # ç¡®ä¿åªé€‰æ•°å€¼ç±»å‹
        numeric_cols = ticker_df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [c for c in feature_cols if c in numeric_cols]

        # ç§»é™¤å¯èƒ½çš„å¹²æ‰°åˆ— (å¦‚ Unnamed)
        feature_cols = [c for c in feature_cols if 'Unnamed' not in c]
        
        # æå–æ•°æ®
        X_values = ticker_df[feature_cols].values
        y_values = ticker_df[TARGET_COL].values
        # ----------------------------------

        # 2. å½’ä¸€åŒ–
        # ç‰¹å¾å½’ä¸€åŒ–
        scaler_X = MinMaxScaler(feature_range=(-1, 1))
        X_scaled = scaler_X.fit_transform(X_values)
        
        # ç›®æ ‡å½’ä¸€åŒ–
        scaler_y = MinMaxScaler(feature_range=(-1, 1))
        y_scaled = scaler_y.fit_transform(y_values.reshape(-1, 1)).flatten()

        # 3. åˆ›å»ºåºåˆ—æ•°æ®
        X, y = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)
        
        if len(X) < 100:
            print(f"Not enough data for {ticker}, skipping.")
            continue

        # 4. åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†
        train_size = int(len(X) * 0.8)
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        # è½¬æ¢ä¸º Tensor
        X_train_t = torch.FloatTensor(X_train).to(device)
        y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)
        X_test_t = torch.FloatTensor(X_test).to(device)
        y_test_t = torch.FloatTensor(y_test).unsqueeze(1).to(device)

        # 5. åˆå§‹åŒ–æ¨¡å‹
        input_dim = X_train.shape[2]
        model = LSTMModel(input_dim=input_dim, hidden_dim=64, num_layers=2).to(device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

        # 6. è®­ç»ƒå¾ªç¯
        model.train()
        for epoch in range(EPOCHS):
            optimizer.zero_grad()
            outputs = model(X_train_t)
            loss = criterion(outputs, y_train_t)
            loss.backward()
            optimizer.step()
            
            if (epoch+1) % 5 == 0:
                print(f"  Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}")

        # 7. ä¿å­˜æ¨¡å‹
        model_path = os.path.join(MODEL_SAVE_DIR, f"lstm_{ticker}.pth")
        torch.save(model.state_dict(), model_path)

        # 8. è¯„ä¼°
        model.eval()
        with torch.no_grad():
            pred_scaled = model(X_test_t).cpu().numpy()
            actual_scaled = y_test_t.cpu().numpy()

        # åå½’ä¸€åŒ–ä»¥è®¡ç®—çœŸå®çš„ RMSE
        pred_actual = scaler_y.inverse_transform(pred_scaled)
        y_actual = scaler_y.inverse_transform(actual_scaled)

        mse = mean_squared_error(y_actual, pred_actual)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_actual, pred_actual)
        r2 = r2_score(y_actual, pred_actual)

        print(f"  {ticker} | RMSE: {rmse:.6f} | MAE: {mae:.6f}")

        # è®°å½•ç»“æœ
        results_list.append({
            'Model': 'LSTM (DL)',
            'Ticker': ticker,
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R2_Score': r2
        })

    # ==========================================
    # 4. ä¿å­˜å¯¹æ¯”ç»“æœ
    # ==========================================
    if results_list:
        new_results_df = pd.DataFrame(results_list)
        
        if os.path.exists(RESULTS_PATH):
            old_results = pd.read_csv(RESULTS_PATH)
            final_df = pd.concat([old_results, new_results_df], ignore_index=True)
        else:
            final_df = new_results_df
        
        final_df.to_csv(RESULTS_PATH, index=False)
        print(f"\nâœ… Updated results saved to {RESULTS_PATH}")
        print(final_df.tail())
    else:
        print("No results generated.")

if __name__ == "__main__":
    run_week9_workflow()